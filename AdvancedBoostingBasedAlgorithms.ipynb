{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdvancedBoostingBasedAlgorithms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8LLLjG1w67"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFh89YMk0Zph"
      },
      "source": [
        "# Lab: Advanced Boosting-based Algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2owTxYuZ0Zpj"
      },
      "source": [
        "## The \"German Credit\" Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSOh_qu0Zpj"
      },
      "source": [
        "### Dataset Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tl5-m3G0Zpk"
      },
      "source": [
        "This dataset has two classes (these would be considered labels in Machine Learning terms) to describe the worthiness of a personal loan: \"Good\" or \"Bad\". There are predictors related to attributes, such as: checking account status, duration, credit history, purpose of the loan, amount of the loan, savings accounts or bonds, employment duration, installment rate in percentage of disposable income, personal information, other debtors/guarantors, residence duration, property, age, other installment plans, housing, number of existing credits, job information, number of people being liable to provide maintenance for, telephone, and foreign worker status.\n",
        "\n",
        "Many of these predictors are discrete and have been expanded into several 0/1 indicator variables (a.k.a. they have been one-hot-encoded).\n",
        "\n",
        "This dataset has been kindly provided by Professor Dr. Hans Hofmann of the University of Hamburg, and can also be found on the UCI Machine Learning Repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WpbQ4yd0Zpl"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ggRDl-0Zpn"
      },
      "source": [
        "## Advanced Boosting-based Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mri8_KBh0Zp2"
      },
      "source": [
        "The next generation of algorithms after Random Forests (that use Bagging, a.k.a. Bootstrap Aggregation) were developed using Boosting, and the first one of these were Gradient Boosted Machines, which are implemented in scikit-learn (http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwUFZKZ60Zp2"
      },
      "source": [
        "Still, in recent years, a number of variations on GBMs have been developed by different research amd industry groups, all of them bringing improvements, both in speed, accuracy and functionality to the original Gradient Boosting algorithms.\n",
        "\n",
        "In no order of preference, these are:\n",
        "1. **XGBoost**: https://xgboost.readthedocs.io/en/latest/\n",
        "2. **CatBoost**: https://tech.yandex.com/catboost/\n",
        "3. **LightGBM**: https://lightgbm.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYSz-FHN0Zp3"
      },
      "source": [
        "If you're using the Anaconda distribution, these are all very easy to install:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8cMmjbu0Zp3"
      },
      "source": [
        "! conda install -c anaconda py-xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w6Q9iqs0Zp3"
      },
      "source": [
        "! conda install -c conda-forge catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVSReVh30Zp3"
      },
      "source": [
        "! conda install -c conda-forge lightgbm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6mCti0A0Zp4"
      },
      "source": [
        "Your task in this optional section of the mini project is to read the documentation of these three libraries, and apply all of them to the \"German Credit\" dataset, just like you did in the case of Decision Trees and Random Forests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7yIF_I0Zp4"
      },
      "source": [
        "The final deliverable of this section should be a table (can be a pandas DataFrame) which shows the accuracy of all the five algorthms taught in this mini project in one place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvBVKES00Zp5"
      },
      "source": [
        "Happy modeling! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACDyLHb90Zp6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}