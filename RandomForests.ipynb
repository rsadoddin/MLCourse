{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForests.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiTErXLa08lj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFh89YMk0Zph"
      },
      "source": [
        "# Lab: Random Forests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2owTxYuZ0Zpj"
      },
      "source": [
        "## The \"German Credit\" Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSOh_qu0Zpj"
      },
      "source": [
        "### Dataset Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tl5-m3G0Zpk"
      },
      "source": [
        "This dataset has two classes (these would be considered labels in Machine Learning terms) to describe the worthiness of a personal loan: \"Good\" or \"Bad\". There are predictors related to attributes, such as: checking account status, duration, credit history, purpose of the loan, amount of the loan, savings accounts or bonds, employment duration, installment rate in percentage of disposable income, personal information, other debtors/guarantors, residence duration, property, age, other installment plans, housing, number of existing credits, job information, number of people being liable to provide maintenance for, telephone, and foreign worker status.\n",
        "\n",
        "Many of these predictors are discrete and have been expanded into several 0/1 indicator variables (a.k.a. they have been one-hot-encoded).\n",
        "\n",
        "This dataset has been kindly provided by Professor Dr. Hans Hofmann of the University of Hamburg, and can also be found on the UCI Machine Learning Repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMNDjWTW0Zpx"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knd8odUG0Zpy"
      },
      "source": [
        "Decision Tree algorithms also have certain undesireable properties. Mainly the have low bias, which is good, but tend to have high variance - which is *not* so good (more about this problem here: https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyEFugLI0Zpz"
      },
      "source": [
        "Noticing these problems, the late Professor Leo Breiman, in 2001, developed the Random Forests algorithm, which mitigates these problems, while at the same time providing even higher predictive accuracy than the majority of Decision Tree algorithm implementations. While the curriculum contains two excellent lectures on Random Forests, if you're interested, you can dive into the original paper here: https://link.springer.com/content/pdf/10.1023%2FA%3A1010933404324.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyez6dR40Zpz"
      },
      "source": [
        "In the next part of this lab, your are going to use the same \"German Credit\" dataset to train, tune, and measure the performance of a Random Forests model. You will also see certain functionalities that this model, even though it's a bit of a \"black box\", provides for some degree of interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0HOGXjJ0Zpz"
      },
      "source": [
        "First, let's build a Random Forests model, using the same best practices that you've used for your Decision Trees model. You can reuse the things you've already imported there, so no need to do any re-imports, new train/test splits, or loading up the data again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8MwjgnV0Zpz"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xpfpKOm0Zp0"
      },
      "source": [
        "# Your code here! :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJR0c02P0Zp0"
      },
      "source": [
        "As mentioned, there are certain ways to \"peek\" into a model created by the Random Forests algorithm. The first, and most popular one, is the Feature Importance calculation functionality. This allows the ML practitioner to see an ordering of the importance of the features that have contributed the most to the predictive accuracy of the model. \n",
        "\n",
        "You can see how to use this in the scikit-learn documentation (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_). Now, if you tried this, you would just get an ordered table of not directly interpretable numeric values. Thus, it's much more useful to show the feature importance in a visual way. You can see an example of how that's done here: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n",
        "\n",
        "Now you try! Let's visualize the importance of features from your Random Forests model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw-ZViS50Zp0"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g6AqKgt0Zp1"
      },
      "source": [
        "A final method for gaining some insight into the inner working of your Random Forests models is a so-called Partial Dependence Plot. The Partial Dependence Plot (PDP or PD plot) shows the marginal effect of a feature on the predicted outcome of a previously fit model. The prediction function is fixed at a few values of the chosen features and averaged over the other features. A partial dependence plot can show if the relationship between the target and a feature is linear, monotonic or more complex. \n",
        "\n",
        "In scikit-learn, PDPs are implemented and available for certain algorithms, but at this point (version 0.20.0) they are not yet implemented for Random Forests. Thankfully, there is an add-on package called **PDPbox** (https://pdpbox.readthedocs.io/en/latest/) which adds this functionality to Random Forests. The package is easy to install through pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCsIJArR0Zp1"
      },
      "source": [
        "! pip install pdpbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRfJ46zg0Zp1"
      },
      "source": [
        "While we encourage you to read the documentation for the package (and reading package documentation in general is a good habit to develop), the authors of the package have also written an excellent blog post on how to use it, showing examples on different algorithms from scikit-learn (the Random Forests example is towards the end of the blog post): https://briangriner.github.io/Partial_Dependence_Plots_presentation-BrianGriner-PrincetonPublicLibrary-4.14.18-updated-4.22.18.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYjIHoUY0Zp2"
      },
      "source": [
        "So, armed with this new knowledge, feel free to pick a few features, and make a couple of Partial Dependence Plots of your own!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRwZaT5S0Zp2"
      },
      "source": [
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}